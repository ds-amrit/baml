###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "action_items.baml": "class SubTask{\n    id int\n    name string\n}\n\nenum Priority{\n    LOW\n    MEDIUM\n    HIGH\n}\n\nclass Ticket{\n    id int\n    name string\n    description string\n    assignedTo string\n    priority Priority\n    subTasks SubTask[]\n    dependencies int[]\n}\n\nfunction ExtractTasks(meeting_notes: string) -> Ticket[] {\n    // This function extracts tasks from meeting notes\n    // and returns them as a list of Ticket objects.\n    // The implementation is not shown here.\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        You are an expert at analyzing meeting notes and extracting structured action items and tasks.\n        Extract all action items, tasks and subtasks from the meeting transcript below.\n    For each task:\n    - Generate a unique ID\n    - Include who is assigned to it\n    - Set appropriate priority level\n    - Identify subtasks if any\n    - Note any dependencies on other tasks\n\n        {{ctx.output_format}}\n\n        {{_.role(\"user\")}}\n        {{meeting_notes}}\n\n    \"#\n}\n\ntest SimpleTranscript {\n  functions [ExtractTasks]\n  args {\n    meeting_notes #\"\n        Alice: We need to update the website by next week. This is high priority.\n        Bob: I can handle that. I'll need Carol's help with the design though.\n        Carol: Sure, I can help with the design part.\n    \"#\n  }\n}\n\ntest ComplexTranscript {\n  functions [ExtractTasks]\n  args {\n    meeting_notes #\"\n        Alice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.\n        Bob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?\n        Alice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.\n        Carol: I can help with the front-end part of the authentication system.\n        Bob: Great, Carol. I'll handle the back-end optimization then.\n        Alice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.\n        Carol: Is the new billing system already in place?\n        Alice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?\n        Bob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.\n        Alice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.\n        Carol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.\n        Alice: Sounds like a plan. Let's get these tasks modeled out and get started.\n    \"#\n  }\n}",
    "chat.baml": "class MyUserMessage{\n    role \"user\" | \"assistant\"\n    content string\n}\n\nfunction ChatWithLLM(messages: MyUserMessage[]) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Reply to the following messages in a friendly and helpful manner, based on the conversation\n        {% for message in messages %}\n        {{ _.role(message.role) }}: {{ message.content }}\n        {% endfor %}\n\n        Answer:\n    \"#\n}\n\ntest TestName {\n    functions [ChatWithLLM]\n    args   {\n        messages [\n            { role \"user\", content \"Hello, how are you?\" },\n            { role \"assistant\", content \"I'm doing well, thank you!\" } \n        ]\n    }\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.87.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "pii_extractor.baml": "class PIIData {\n  index int\n  dataType string\n  value string\n}\n\nclass PIIExtraction {\n  privateData PIIData[]\n  containsSensitivePII bool @description(\"E.g. if the document contains sensitive PII data make it true else false\")\n}\n\nfunction ExtractPII(document: string) -> PIIExtraction{\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Extract all personally identifiable information (PII) from the given document. Look for items like:\n    - Names\n    - Email addresses\n    - Phone numbers\n    - Addresses\n    - Social security numbers\n    - Dates of birth\n    - Any other personal data\n\n    {{ctx.output_format}}\n\n    {{_.role(\"user\")}}\n    The given document is: \\n\n    {{document}}\n    \"#\n}\n\ntest PIIExtraction {\n    functions [ExtractPII]\n    args{\n        document #\"\n        John Doe, born on 01/01/1990, lives at 123 Main St, Springfield, IL. His email is john_doe@gmail.com\n        and his phone number is (555) 123-4567. He has a social security number of 123-45-6789.\n        Jane Smith, born on 02/02/1985.\n        Her email is jane@smith\n        She is a software engineer at Tech Corp.\n        \"#\n    }\n}\n\ntest EmptyDocument {\n  functions [ExtractPII]\n  args {\n    document \"This document contains no PII data.\"\n  }\n}",
    "rag.baml": "class Response{\n    question string\n    answer string\n}\n\nfunction RAG(question:string, context:string) -> Response {\n    // This function performs a retrieval-augmented generation (RAG) task.\n    // It retrieves relevant context and generates a response based on the question and context.\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Answer the question in full sentences using the provided context.\n        Do not make up an answer. If the information is not provided in the context, say so clearly.\n        \n        Question: {{question}}\n        Context: {{context}}\n\n        {{ctx.output_format}}\n        \n        {{_.role(\"user\")}}\n        {{question}}\n        {{context}}\n        \n        RESPONSE:\n    \"# \n}\n\ntest TestOne {\n  functions [RAG]\n  args {\n    question \"When was SpaceX founded?\"\n    context #\"\n      SpaceX is an American spacecraft manufacturer and space transportation company founded by Elon Musk in 2002.\n    \"#\n  }\n}\n\ntest TestTwo {\n  functions [RAG]\n  args {\n    question \"Where is Fiji located?\"\n    context #\"\n      Fiji is a country in the South Pacific known for its rugged landscapes, palm-lined beaches, and coral reefs with clear lagoons.\n    \"#\n  }\n}",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "spam_classifier.baml": "enum UserMessage{\n    SPAM\n    NOT_SPAM\n}\n\nfunction ClassifyText(text: string) -> UserMessage {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        classify the following message:\n            \n        \n        {{ctx.output_format}}\n\n        {{_.role(\"user\")}}\n        text: {{text}}\n        \n    \"#\n}\n\ntest BasicSpamTest {\n    functions [ClassifyText]\n    args {\n        text \"Congratulations! You've won a $1000 gift card. Click here to claim your prize.\"\n    }\n}\n\ntest BasicNotSpamTest {\n    functions [ClassifyText]\n    args {\n        text \"Hey, do you want to grab lunch tomorrow?\"\n    }\n}\n\nenum TicketLabel {\n  ACCOUNT\n  BILLING\n  GENERAL_QUERY\n}\nclass TicketClassification {\n  labels TicketLabel[]\n}\n\nfunction ClassifyTicket(ticket: string) -> TicketClassification {\n  client \"openai/gpt-4o-mini\"\n  prompt #\"\n    You are a support agent at a tech company. Analyze the support ticket and select all applicable labels.\n    {{ ctx.output_format }}\n    {{ _.role(\"user\") }}\n    \n    {{ ticket }}\n  \"#\n}\n\ntest ClassifyTicketSingleLabel {\n  functions [ClassifyTicket]\n  args {\n    ticket \"I want to increase my salary\"\n  }\n}\ntest ClassifyTicketMultiLabel {\n  functions [ClassifyTicket]\n  args {\n    ticket \"My account is locked and I can't access my billing information\"\n  }\n}\n\n",
    "tools.baml": "class WeatherAPI {\n  city string\n  time string @description(\"Current time in ISO8601 format\")\n  temperature string @description(\"Temperature in Celsius\")\n}\n\nclass CalculatorAPI {\n    answer string @description(#\" The answer to the calculation.\n        The answer should be a string, even if it is a number.\n        For example, if the answer is 42, it should be returned as \"42\".\n        Use BODMAS to calculate the answer.\n    \"#)\n}\n\n\n\nfunction SelectTool(message: string) -> WeatherAPI | CalculatorAPI {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Given a message, extract info.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} {{ message }}\n  \"#\n}",
}

def get_baml_files():
    return file_map